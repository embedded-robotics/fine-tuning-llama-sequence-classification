{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/DRG-LLaMA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_device_num=1\n",
    "torch.cuda.set_device(gpu_device_num)\n",
    "torch.cuda.current_device()\n",
    "device = torch.device(f\"cuda:{gpu_device_num}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label_pd = pd.read_csv(\"data/id2label.csv\")\n",
    "drg_desc_pd = pd.read_csv(\"data/drg_34_dissection.csv\")\n",
    "drg_desc_pd = drg_desc_pd.rename(columns={\"DRG\": \"drg_34_code\"})\n",
    "merged_df = id2label_pd.merge(drg_desc_pd, how='left', on=\"drg_34_code\")\n",
    "merged_df = merged_df[['drg_34_code', 'label', 'Description']].set_index('label', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_id = \"experiments/7b-512-4-2e-05-right-April-14-14-52\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(checkpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(config.base_model_name_or_path,\n",
    "                                           model_max_length=512,\n",
    "                                           cache_dir=\"/data/mn27889/.cache/huggingface\")\n",
    "tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.11it/s]\n",
      "Some weights of the model checkpoint at baffo32/decapoda-research-llama-7b-hf were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at baffo32/decapoda-research-llama-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "inference_model = LlamaForSequenceClassification.from_pretrained(config.base_model_name_or_path,\n",
    "                                                       num_labels=738,\n",
    "                                                       load_in_8bit=True,\n",
    "                                                       torch_dtype=torch.float16,\n",
    "                                                       cache_dir=\"/data/mn27889/.cache/huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = PeftModel.from_pretrained(inference_model, checkpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(\n",
       "          in_features=4096, out_features=738, bias=False\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=8, out_features=738, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(\n",
       "            in_features=4096, out_features=738, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=738, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_model = inference_model.to(device)\n",
    "inference_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "___ yo male with history of EtOH abuse (withdrawal seizures) who \n",
    "presents with exertional chest pain X 48 hrs and increasing \n",
    "depression, suicidal ideation X 6 weeks resulting in pt not \n",
    "taking his medications for the last 5 days.\n",
    "\n",
    "Chest Pain - Patient was a poor historian. It was unclear if the \n",
    "chest pain was cardiac or of other etiology. Differential \n",
    "diagnosis included cardiac (stable angina, hypertensive urgency) \n",
    "vs. GERD vs. Psychiatric vs. Musculoskeletal. Cardiac enzymes \n",
    "were negative X3. However, patient has a number of coronary \n",
    "artery disease risk factors, including father with fatal MI at \n",
    "___ years old, personal medical history of uncontrolled \n",
    "hypertension, obesity and alcohol abuse. Patient's last ECHO was \n",
    "done in ___ which showed preserved cardiac function \n",
    "(LVEF 60%) but no previous stress test. A nuclear stress test \n",
    "was ordered for the second day of hospitalization and patient \n",
    "started on Atorvastatin 20mg. Patient, though, refused the \n",
    "stress test and ate breakfast the morning of his scheduled \n",
    "stress, stating that he did not need the test. LFTs came back \n",
    "with slightly elevated AST/ALT but Lipid Panel was within normal \n",
    "limits. Atorvastatin was discontinued. On the third day of \n",
    "admission, the recommended nuclear stress was re-addressed with \n",
    "patient, who again refused. On the day of discharge, patient had \n",
    "recurrent chest pain which, per his report, was similar to the \n",
    "pain from the day of admission. Pt refused medical interventions \n",
    "at that time, including an EKG or nuclear stress. It was felt, \n",
    "at that time, that the patient was competent to make this \n",
    "decision as he reiterated understanding of the risks of refusing \n",
    "treatment, including myocardial infarction, stroke, other \n",
    "serious cardiac events. It was felt that patient's chest pain \n",
    "was likely due to antihypertensive non-compliance. \n",
    "\n",
    "Hypertension - Patient elicited non-compliance with medications \n",
    "X5\n",
    "days, likely longer. Hypertension also exacerbated by EtOH \n",
    "abuse. He was restarted on Atenolol 50mg BID, Amlodipine 5mg \n",
    "daily which he started refusing the second day of admission. \n",
    "Blood pressure was mildly decreased with the Valium as part of \n",
    "the CIWA protocol, which he routinely triggered during the day. \n",
    "Blood pressure went from SBP130s on the day of admission (after \n",
    "taking medications) to 180/dopplerable on the day of discharge. \n",
    "Patient was able to state his understanding that non-compliance \n",
    "with these medications could increase his risk for stroke and \n",
    "was likely contributing to his chest pain as hypertensive \n",
    "urgency. \n",
    "\n",
    "EtOH abuse - Patient's last drink was ___ pm on ___. \n",
    "Patient was intoxicated in the ED and starting to withdraw \n",
    "(symptoms of anxiety, tremulousness). Has history of seizures ___ \n",
    "year ago), denies hallucinations/delirium tremens. Patient was \n",
    "started on Diazepam ___ PO per CIWA protocol (>=10, q4hrs). \n",
    "On the day of discharge, he triggered CIWA with score of 14 that \n",
    "morning but had not triggered over the 8 hours overnight. \n",
    "Patient was given 1L banana bag the first evening but switched \n",
    "to PO B12, folate, multivitamin per patient's request. \n",
    "Psychiatry and Social Work were consulted. They recommended \n",
    "Section 35 for the patient as he had failed multiple \n",
    "rehabilitation attempts and had been hospitalized multiple times \n",
    "for similar symptoms (suicidal ideation, depression, alcohol \n",
    "withdrawal). It was felt that the patient's primary issue is his \n",
    "alcohol abuse and that his elicitation of suicidal thoughts is \n",
    "often for secondary gain (please refer to patient's Section 35 \n",
    "for full explanation). \n",
    "\n",
    "Depression/Suicidal Ideation: Patient contracted for safety \n",
    "throughout his stay. Throughout this admission, he elicited \n",
    "passive suicidal ideation (die by heart attack, overdose, hit by \n",
    "train etc.). Patient had a 1:1 sitter throughout this admission. \n",
    "Patient was continued on Ambien 5mg before bed as needed for \n",
    "insomnia. He was also started on Risperidal BID PRN per \n",
    "Psychiatry recommendations, which was not taken throughout this \n",
    "stay. \n",
    "\n",
    "Diarrhea/Groin Rash: Patient c/o diarrhea during this hospital \n",
    "admission which was not witnessed by nursing but improved with \n",
    "Immodium. Patient also described a pruritic groin rash from the \n",
    "gel used for ___ ultrasound. He deferred physician examination \n",
    "of the area, however, with the understanding that without \n",
    "allowing an MD to examine the rash, he would not be given \n",
    "topical medication for it. \n",
    "\n",
    "GERD: Stable per patient, who was continued on Pantoprazole 40mg \n",
    "daily. \n",
    "\n",
    "Chronic lower back pain: Stable. Patient was continued on \n",
    "Neurontin (300mg at 8 am, 300mg at 2 pm, 900mg. He was also \n",
    "continued on Ibuprofen 600mg every 8 hours as needed.\n",
    "\n",
    "Gout: Stable, previously affected right great toe. No \n",
    "Indomethicin was started. \n",
    "\n",
    "FEN: Regular, cardiac diet\n",
    "\n",
    "PPx: Subcutaneous heparin, bowel regimen (docusate/senna PRN),\n",
    "ibuprofen 600mg q8hrs PRN pain.\n",
    "\n",
    "Communication: Patient, HCP: ___ (brother)\n",
    "\n",
    "Code: Full (confirmed with patient)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(text, truncation=True, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    output = inference_model(**input)\n",
    "\n",
    "prediction_label = output.logits.argmax(dim=-1).item()\n",
    "del output\n",
    "del input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313: CHEST PAIN\n"
     ]
    }
   ],
   "source": [
    "predicted_drg_code = merged_df.iloc[prediction_label]['drg_34_code']\n",
    "predicted_drg_desc = merged_df.iloc[prediction_label]['Description']\n",
    "print(f\"{predicted_drg_code}: {predicted_drg_desc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRG-LLaMA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
